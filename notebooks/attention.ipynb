{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import graphviz as gv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's talk about attention\n",
    "\n",
    "# Transformers: Executive Summary\n",
    "\n",
    "Let's start with defining what our transformer is. A transformer is a function $\\mathcal{F}$ where:\n",
    "\n",
    "$$\\mathcal{F}(W_i, C_i; x) \\to {C_i}', x'$$\n",
    "\n",
    "A model is a function which starts with a small lookup table (on the order of\n",
    "$10^6$ parameters) and then applies the transformer function $\\mathcal{F}$ a\n",
    "number of times:\n",
    "\n",
    "$$\\mathcal{M}(\\mathbf{W}, \\mathbf{C}, x) \\to \\mathbf{C'} + x'$$\n",
    "\n",
    "Then $x'$ is passed through a linear $\\mathcal{F}_\\text{cls}$ function to get\n",
    "the output, which is a probability distribution over the vocabulary. We sample\n",
    "from this distribution to get the next word, and then we repeat the process.\n",
    "\n",
    "An *inference farm* is a a service that provides $\\mathcal{F}$-as-a-service with\n",
    "extreme parallelism. Of course, we should not count out\n",
    "$\\mathcal{F}_\\text{cls}$, which also has some computational overhead; but's for\n",
    "now, let's make two simplifying assumptions:\n",
    "\n",
    "- Function $\\mathcal{F}_\\text{cls}$ and the subsequent sampling are free.\n",
    "- There's only one $\\mathbf{W}$ (i.e., the model is fixed).\n",
    "\n",
    "Okay, let's turn our focus into the inside of the transformer $\\mathcal{F}$. It\n",
    "has three stages:\n",
    "\n",
    "- Pre-Attention\n",
    "- Attention\n",
    "- Post-Attention\n",
    "\n",
    "The pre-attention step is a function $\\mathcal{F}_\\text{pre}$ that takes only\n",
    "$W_i$ and $x$ as input, and produce some intermediate values:\n",
    "\n",
    "$$\\mathcal{F}_\\text{pre}(W_i; x) \\to x', a, b$$\n",
    "\n",
    "Size of $a$ and $b$ are almost the same as the size of $x$. Next, we get to\n",
    "attention which as input receives $C_i$, $A$, $B$ and $x$ (no $W_i$):\n",
    "\n",
    "$$\\mathcal{F}_\\text{att}(C_i; a, b, x) \\to C_i', x'$$\n",
    "\n",
    "Finally, we have the post-attention step, which takes $C_i$, $x$ and $x'$ as\n",
    "input:\n",
    "\n",
    "$$\\mathcal{F}_\\text{post}(W_i, C_i; x, x') \\to C_i', x_\\text{out}$$\n",
    "\n",
    "We identify $x$ as one-dimensional array of $d$ floats, where $d$ is a constant\n",
    "per Model $\\mathcal{M}$. Similarly, $W_i$ is a one-dimensional array of $L_W$\n",
    "floats. This is also a constant per Model $\\mathcal{M}$ (\"parameters\"). $C_i$\n",
    "starts as an empty array, and with each invocation of $\\mathcal{F}$, it grows by\n",
    "$2d'$ floating-point numbers. There's a maximum on how much $C_i$ can grow, and\n",
    "it's $L_C$ floats (wide range of $2^10--2^{20} and beyond). \n",
    "\n",
    "# Pre-Attention\n",
    "\n",
    "Long story short, the bulk of computations in pre-attetion is two matrix\n",
    "multiplications. The first one is $d \\times d$ matrix, and the second one is a\n",
    "$d \\times 2d'$ matrix (no data dependency between the two). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"190pt\" height=\"85pt\"\n",
       " viewBox=\"0.00 0.00 189.59 84.69\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 80.69)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-80.69 185.59,-80.69 185.59,4 -4,4\"/>\n",
       "<!-- f_pre -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>f_pre</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"34.45\" cy=\"-38.35\" rx=\"34.39\" ry=\"34.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"34.45\" y=\"-34.65\" font-family=\"Times,serif\" font-size=\"14.00\">f_pre</text>\n",
       "</g>\n",
       "<!-- f_post -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>f_post</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"143.24\" cy=\"-38.35\" rx=\"38.19\" ry=\"38.19\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.24\" y=\"-34.65\" font-family=\"Times,serif\" font-size=\"14.00\">f_post</text>\n",
       "</g>\n",
       "<!-- f_pre&#45;&gt;f_post -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>f_pre&#45;&gt;f_post</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.94,-38.35C77.12,-38.35 86.03,-38.35 94.72,-38.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.77,-41.85 104.77,-38.35 94.77,-34.85 94.77,-41.85\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7a095eefc190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gv.Digraph()\n",
    "g.graph_attr['rankdir'] = 'LR'\n",
    "g.node_attr['shape'] = 'circle'\n",
    "\n",
    "g.edge('F' )g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "A[Hard] -->|Text| B(Round)\n",
    "B --> C{Decision}\n",
    "C -->|One| D[Result 1]\n",
    "C -->|Two| E[Result 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
